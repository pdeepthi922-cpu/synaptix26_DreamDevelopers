PROJECT NAME: SkillBridge
Tagline: Intelligent, Fair, and Transparent Internship & Project Matching

================================================================================
BACKEND IMPLEMENTATION PLAN
================================================================================

Generated: 2026-02-27
Last Updated: 2026-02-27 (Added Python NLP microservice for resume parsing & scoring)

================================================================================
PROJECT SUMMARY
================================================================================

SkillBridge is a fairness-aware internship and project matching platform that
evaluates candidates using a weighted skill competency algorithm and generates
a transparent Match Score. It serves two user types — candidates looking for
opportunities, and recruiters posting listings. Success means candidates get
honest eligibility feedback with actionable gap guidance, and recruiters get
a clean ranked view of applicants.

================================================================================
TECH STACK DECISION
================================================================================

--- NODE.JS SERVICE (Main API) ---

Layer          | Choice                    | Why
---------------|---------------------------|--------------------------------------------------
Runtime        | Node.js (v20 LTS)         | Primary API runtime; large ecosystem, async I/O
Framework      | Express.js                | Minimal, well-documented, no magic — easy to debug
ORM            | Prisma                    | Type-safe queries, great migration tooling, works with PostgreSQL
Database       | PostgreSQL                | Relational model ideal for scoring/ranking/application data
Auth           | JWT + bcrypt              | Stateless tokens = no session storage needed
Validation     | Zod                       | Schema-first validation, pairs perfectly with Prisma models
Hosting        | Render (free tier)        | Supports PostgreSQL + Node.js, zero-config deploys

--- PYTHON SERVICE (NLP Microservice) ---

Layer          | Choice                    | Why
---------------|---------------------------|--------------------------------------------------
Runtime        | Python 3.11               | Best-in-class NLP ecosystem; no competition here
Framework      | FastAPI                   | Async, fast, automatic OpenAPI docs, easy to deploy
Resume Parsing | pdfminer.six              | More reliable PDF text extraction than pdf-parse
NLP Engine     | spaCy (en_core_web_md)    | Named entity recognition, token classification, production-grade
Skill Taxonomy | Custom CSV / dict         | Maps frameworks to parent skills (Flask->Python, React->JS, etc.)
Scoring Engine | Pure Python               | Cleaner logic, easier to unit test, leverage NLP-expanded skills
Hosting        | Render (free tier)        | Second Render service on same account, stays free

ASSUMPTION: Resumes are parsed in-memory on upload — the original PDF
is NOT stored on disk. The Node.js API forwards the raw PDF bytes to
the Python service via an internal HTTP call, receives structured JSON,
and saves the extracted data to PostgreSQL.

NOTE ON COMMUNICATION: The Python service is INTERNAL ONLY. The
frontend never calls it directly. Only the Node.js API calls it.

================================================================================
ARCHITECTURE OVERVIEW
================================================================================

The system runs as two services: a Node.js/Express main API (all business
logic, auth, data persistence) and a Python/FastAPI microservice (resume
parsing with NLP + Match Score calculation). The frontend only ever talks
to the Node.js API — the Python service is a private internal dependency.

[React/Vite Frontend]
        |  HTTPS REST
        v
[Node.js / Express API]  <-- ALL frontend calls go here
   |-- /auth          --> register, login (JWT issued)
   |-- /candidates    --> profile CRUD, resume upload (proxied to Python)
   |-- /recruiters    --> company profile CRUD
   |-- /postings      --> internships & projects (CRUD)
   |-- /scores        --> calls Python to score, caches result in DB
   |-- /applications  --> apply, withdraw, list
   |-- /rankings      --> ranked candidate list per posting
   +-- /notifications --> in-app notify, badge count
        |                       |
        v                       | internal HTTP (not exposed publicly)
  [Prisma ORM]                  v
        |           [Python / FastAPI Microservice]
        v              |-- POST /parse-resume
  [PostgreSQL DB]      |       pdfminer.six + spaCy NER
                       |       returns: { name, email, phone,
                       |         skills: [{name, inferredLevel}] }
                       |
                       +-- POST /calculate-score
                               spaCy skill expansion via taxonomy
                               (Flask->Python, React->JS, etc.)
                               returns: { score, breakdown, gaps,
                                 projectedScore }

================================================================================
DATABASE SCHEMA (Key Models)
================================================================================

User            --> id, email, passwordHash, role (CANDIDATE | RECRUITER), createdAt
CandidateProfile--> userId, name, phone, location, linkedinUrl
Skill           --> id, candidateId, skillName, proficiency (1-5)
Posting         --> id, recruiterId, type (INTERNSHIP | PROJECT), title,
                    description, stipend?, duration?, deadline, location,
                    remote, createdAt
PostingSkill    --> id, postingId, skillName, weight (1-5)
Application     --> id, candidateId, postingId, appliedAt, withdrawn (bool)
MatchScore      --> id, candidateId, postingId, score (float),
                    isStale (bool), calculatedAt
Notification    --> id, candidateId, postingId, message, read (bool), createdAt

================================================================================
STEP-BY-STEP IMPLEMENTATION PLAN
================================================================================

------------------------------------------------------------------------
PHASE 1: FOUNDATION
Goal: Get a running server with a database connected and basic structure in place.
------------------------------------------------------------------------

STEP 1: Project Scaffold & Environment Setup

  [Node.js Service]
  What:   Initialize Node.js project. Install all dependencies:
          express, prisma, @prisma/client, bcrypt, jsonwebtoken, dotenv,
          zod, cors, multer, axios.
          NOTE: Remove pdf-parse — parsing now lives in Python service.
          Set up folder structure:
            src/routes/, src/middleware/, src/services/, src/utils/
  Done when: `npm run dev` starts on port 5000 and GET /health
              returns { status: "ok" }

  [Python Service]
  What:   Create a separate folder (e.g., /python-service/).
          Initialize: python -m venv venv && pip install fastapi
          uvicorn pdfminer.six spacy requests
          Download spaCy model: python -m spacy download en_core_web_md
          Create skill_taxonomy.py with the framework-to-skill map.
          Create main.py with two routes: POST /parse-resume and
          POST /calculate-score.
  Done when: `uvicorn main:app --reload` starts on port 8000 and
              GET /health returns { "status": "ok" }

  Why:    Clean separation from day one prevents dependency and
          deployment confusion later.

STEP 2: Database Schema Design (Prisma)
  What:   Define all Prisma models — User, CandidateProfile,
          RecruiterProfile, Skill (with proficiency), Posting (internship
          or project), PostingSkill (required skill with weight),
          Application, MatchScore (with isStale boolean), Notification.
          Run: npx prisma migrate dev --name init
  Why:    Getting the schema right first avoids painful migrations
          mid-build.
  Done when: `npx prisma migrate dev --name init` runs without errors
              and all tables exist in the DB.

STEP 3: Auth Endpoints
  What:   Build POST /auth/signup (validate role, hash password, create
          User + profile stub) and POST /auth/login (verify credentials,
          return signed JWT). Add JWT middleware that attaches req.user
          on protected routes.
  Why:    Every subsequent endpoint depends on knowing who is calling
          and their role.
  Done when: POST /auth/signup creates a user in DB; POST /auth/login
              returns a valid JWT; hitting a protected route without a
              token returns 401.

STEP 4: Onboarding Endpoints + Python Resume Parser

  [Python Service — build this first]
  What:   Implement POST /parse-resume in FastAPI.
          - Accept raw PDF bytes (multipart/form-data)
          - Extract text using pdfminer.six
          - Run spaCy NER to detect PERSON, ORG, GPE entities
          - Use regex patterns to extract email, phone, LinkedIn URL
          - Match known skill keywords against the text (case-insensitive)
          - For each detected framework/lib, expand via skill_taxonomy.py
            e.g., "Flask" found --> add Python, REST APIs to skills list
          - Infer a rough proficiency (1-3) based on context clues
            ("proficient in", "experienced with", "familiar with")
          - Return structured JSON:
            { name, email, phone, location, linkedinUrl,
              skills: [{ skillName, proficiency }],
              projects: [...], experience: [...] }

  [Node.js Service]
  What:   PUT /candidates/onboarding — accepts personal details + skills
          array (manual entry flow).
          POST /candidates/resume — accepts PDF via multer, forwards raw
          bytes to Python service via axios call to
          PYTHON_SERVICE_URL/parse-resume, returns the parsed JSON
          directly to the frontend for candidate review & editing.
          PUT /recruiters/onboarding — accepts company name and size.
  Why:    Onboarding gates dashboard access. The Python parser does the
          heavy lifting; Node.js is just the gateway and data persister.
  Done when: POST a real PDF resume --> receive structured JSON with
              name, email, and expanded skills (e.g., Flask appears in
              resume --> response includes Python in skills list).
              All confirmed data saves correctly to PostgreSQL.

------------------------------------------------------------------------
PHASE 2: CORE FEATURES
Goal: Implement the main business logic and API endpoints.
------------------------------------------------------------------------

STEP 5: Postings CRUD
  What:   POST /postings (recruiter only),
          GET /postings?type=internship&search=&page= (paginated, public),
          GET /postings/:id,
          PUT /postings/:id (recruiter owner only),
          DELETE /postings/:id (recruiter owner only).
          Seed 5-10 sample postings at this step.
  Why:    Candidates can't do anything meaningful without postings in
          the system.
  Done when: Recruiter can create a posting with skills+weights; paginated
              listing returns 10 per page; search by title/keyword works.

STEP 6: Match Score Engine (Python) + Score Endpoint (Node.js)

  [Python Service — build this first]
  What:   Implement POST /calculate-score in FastAPI.
          Request body:
            { candidateSkills: [{skillName, proficiency}],
              postingSkills:   [{skillName, weight}] }

          Core algorithm:
            1. Expand candidateSkills using skill_taxonomy.py
               e.g., candidate has Flask (prof 4)
                     --> also treat Python as known (prof 4)
            2. For each required postingSkill:
               - Find match in expanded candidate skills (lowercase)
               - proficiency = match.proficiency OR 0 if missing
               - contribution = proficiency * weight
               - earned += contribution
               - maxPossible += 5 * weight
            3. score = (earned / maxPossible) * 100
            4. Build breakdown list (per skill: proficiency, weight,
               contribution, matched: true/false)
            5. Build gaps list (missing or low-proficiency skills)
               For gaps, suggest sub-skills from taxonomy to learn
            6. Compute projectedScore assuming all gaps fixed at prof 5

          Return:
            { score, breakdown, gaps, projectedScore }

  [Node.js Service]
  What:   POST /scores/check/:postingId:
          1. Load candidate skills from DB
          2. Load posting required skills from DB
          3. Check MatchScore cache — if fresh (isStale=false), return it
          4. If stale or missing: call Python service /calculate-score
             via axios, receive result, upsert into MatchScore table
          5. Return scorecard to frontend
  Why:    Python handles NLP-aware scoring; Node.js handles caching,
          persistence, and the API surface. Clean separation of concerns.
  Done when: POST /scores/check/:postingId correctly returns 48% for the
              spec example. Flask in resume correctly causes Python
              expansion to also match a Python-required posting.

STEP 7: Score Caching & Staleness
  What:   On PUT /candidates/profile (any profile update), use a Prisma
          transaction to set isStale = true on all MatchScore rows for
          that candidate. On POST /scores/check/:postingId, if a fresh
          cached score exists, return it; otherwise recalculate and
          upsert.
  Why:    Recalculating on every page view is wasteful; but stale scores
          after profile edits are misleading.
  Done when: Update a candidate's skills --> check eligibility for an old
              posting --> confirm the score recalculates rather than
              returning the stale cached value.

STEP 8: Applications
  What:   POST /applications/:postingId (apply — only if score >= 80%),
          DELETE /applications/:postingId (withdraw — only before deadline),
          GET /applications/mine (candidate's applied list).
  Why:    Applications are gated by the score >= 80% rule — enforce this
          server-side, not just on the frontend.
  Done when: Applying with score < 80% returns 403 Forbidden; withdrawal
              before deadline succeeds; withdrawal after deadline returns
              a clear error.

STEP 9: Rankings
  What:   GET /postings/:id/rankings — returns all candidates who have a
          MatchScore for this posting, sorted by score descending,
          including candidate name, score, and application status.
  Why:    Rankings are semi-public by design; both candidates and
          recruiters need this.
  Done when: After 3 candidates check eligibility for a posting, the
              ranking returns all 3 sorted correctly.

STEP 10: Notifications
  What:   POST /notifications/notify/:candidateId/:postingId
            (recruiter only),
          GET /notifications/mine (candidate — returns unread count + list),
          PUT /notifications/:id/read.
  Why:    Recruiters need a simple way to signal interest to candidates
          without email.
  Done when: Recruiter POSTs a notification --> candidate's
              GET /notifications/mine returns it with read: false and
              unread count = 1.

STEP 11: Recommendations
  What:   GET /candidates/recommendations — fetches candidate's skill
          names, returns postings whose PostingSkill list overlaps (simple
          IN query). If no skills: return 10 most recent postings.
          Separate results by type (internship / project).
  Why:    The dashboard needs something meaningful on first login.
  Done when: A candidate with skills ["React", "Node.js"] gets back
              postings that require either skill, ranked by overlap count.

------------------------------------------------------------------------
PHASE 3: SECURITY & VALIDATION
Goal: Make it production-safe — auth, input validation, error handling.
------------------------------------------------------------------------

STEP 12: Input Validation with Zod
  What:   Add Zod schemas for every request body (signup, login,
          onboarding, posting creation, etc.). Create a validate(schema)
          middleware that returns 400 with a clear error message on
          failure.
  Why:    Without this, any malformed request can crash the server or
          corrupt the DB.
  Done when: Sending proficiency: 10 (out of 1-5 range) returns 400
              with a descriptive validation error.

STEP 13: Role-Based Access Control
  What:   Add requireRole('CANDIDATE') and requireRole('RECRUITER')
          middleware. Apply to every endpoint — candidates cannot POST
          postings, recruiters cannot apply, etc.
  Why:    Role confusion is a silent security hole that's easy to miss
          without explicit enforcement.
  Done when: A recruiter JWT calling POST /applications/:postingId
              returns 403.

STEP 14: Global Error Handling
  What:   Add a global Express error handler
          (app.use((err, req, res, next) => ...)).
          Normalize all errors to { error: "message" } format.
          Never expose stack traces in production (NODE_ENV=production).
  Why:    Unhandled errors crash the server or leak implementation
          details to attackers.
  Done when: Triggering a DB constraint violation returns a clean JSON
              error, not an HTML crash page.

STEP 15: Ownership Guards
  What:   Before PUT /postings/:id or DELETE /postings/:id, verify
          posting.recruiterId === req.user.id. Before DELETE
          /applications/:postingId, verify the application belongs to
          the requesting candidate.
  Why:    Without this, any authenticated recruiter can edit or delete
          another recruiter's posting.
  Done when: Recruiter A trying to delete Recruiter B's posting returns
              403.

------------------------------------------------------------------------
PHASE 4: TESTING & POLISH
Goal: Confidence that it works and won't break in production.
------------------------------------------------------------------------

STEP 16: Unit Test Both Services

  [Python Service — pytest]
  What:   Write pytest tests for the scoring engine covering:
          - Exact spec example: Python(4)×5 + SQL(3)×3 / max = 48%
          - 100% match, 0% match, empty skills on either side
          - Taxonomy expansion: Flask in candidate --> matches Python posting
          - Case-insensitivity: "REACT" matches "react"
          - projectedScore accuracy when all gaps are filled
          Run: pytest python-service/tests/

  [Node.js Service — Jest + Supertest]
  What:   Test the /scores/check/:postingId route to confirm:
          - Fresh cache is returned without calling Python service
          - Stale cache triggers a new Python service call
          - 403 returned if candidate tries to apply with score < 80%
  Why:    The score is the entire product's value proposition — every
          edge case must be verified in both services.
  Done when: `pytest` passes all Python scoring tests including the 48%
              spec example and Flask->Python taxonomy expansion.
              `npm test` passes all Node.js integration tests.

STEP 17: Integration-Test Critical API Routes
  What:   Use Supertest + Jest to test the 5 most critical flows:
          signup->login, onboarding, post-posting, check-eligibility
          (score caching), apply (gate enforcement).
          Use a separate TEST_DATABASE_URL in .env.test.
  Why:    Manual testing of auth + DB interactions is slow and unreliable
          across environments.
  Done when: All integration tests pass against the test DB.

STEP 18: Environment & Deployment Prep
  What:   Finalize .env.example (all required vars documented).
          Configure render.yaml or Procfile.
          Run `prisma migrate deploy` on Render build hook.
          Set NODE_ENV=production.
  Why:    A broken deploy on demo day is catastrophic.
  Done when: The app deploys cleanly to Render, GET /health returns 200,
              and a full signup->apply flow works on the live URL.

================================================================================
ENVIRONMENT VARIABLES (.env)
================================================================================

Required variables for each service. Create .env files in each service
folder root (never commit them — add to .gitignore).

--- NODE.JS SERVICE  (/node-service/.env) ---

# Server
PORT=5000
NODE_ENV=development

# Database
DATABASE_URL="postgresql://USER:PASSWORD@HOST:PORT/DBNAME?schema=public"

# JWT
JWT_SECRET="your-strong-secret-here"
JWT_EXPIRES_IN="7d"

# Python Microservice URL (change this to update which Python service to use)
PYTHON_SERVICE_URL=http://localhost:8000

# (Test only)
TEST_DATABASE_URL="postgresql://USER:PASSWORD@HOST:PORT/DBNAME_test?schema=public"

--- PYTHON SERVICE  (/python-service/.env) ---

# Server
PYTHON_PORT=8000
ENVIRONMENT=development

NOTE: The Python service reads these via python-dotenv.
      Install it: pip install python-dotenv

--- HOW TO SWITCH BACKEND TARGETS ---

To point Node.js at a different Python service (e.g., deployed on Render):
  Change PYTHON_SERVICE_URL in the Node.js .env only.
  No source code changes needed.

Create .env.example files in both service folders with the same keys
but placeholder values so teammates know exactly what to configure.

================================================================================
COMMON PITFALLS TO AVOID
================================================================================

1. Storing proficiency/weight as strings
   Fix: Declare them as Int in Prisma schema; Zod should coerce if
        coming from form data.

2. Not invalidating stale scores on profile update
   Fix: Use a Prisma $transaction that updates profile AND sets
        isStale=true atomically.

3. Forgetting the deadline check on withdrawals
   Fix: Store deadline as DateTime in PostgreSQL; compare with
        new Date() server-side — never trust the client.

4. Skill name matching is case-sensitive
   Fix: Lowercase ALL skill names in the taxonomy dict, at DB insert
        time in Node.js, AND before comparison in the Python scorer.

5. Python service is down when Node.js calls it
   Fix: Wrap every axios call to PYTHON_SERVICE_URL in a try/catch.
        If the Python service is unreachable, return 503 with a clear
        message: "Scoring service temporarily unavailable."
        Never let a Python failure crash the Node.js API.

6. spaCy model not downloaded on the server
   Fix: Add `python -m spacy download en_core_web_md` to the Python
        service's Render build command. Without this, FastAPI crashes
        on startup with a model not found error.

7. Taxonomy expansion causing false-positive skill matches
   Fix: Keep the taxonomy conservative — only add a parent skill if
        the candidate's proficiency with the child is >= 3. Don't
        infer Python from Flask if the candidate barely knows Flask.

================================================================================
ESTIMATED EFFORT (Solo Developer, Intermediate Level)
================================================================================

Phase 1: Foundation        (Steps 1-4)   ~2 days   (+0.5 day for Python setup)
Phase 2: Core Features     (Steps 5-11)  ~3-4 days (Python scorer adds ~0.5 day)
Phase 3: Security          (Steps 12-15) ~1 day
Phase 4: Testing & Polish  (Steps 16-18) ~1.5 days (pytest adds time)
-----------------------------------------------------------------------
TOTAL                                    ~8-9 days

HACKATHON NOTE:
  - Build and verify the Python service (health check + /parse-resume)
    on Day 1 before touching any Node.js feature code. If it breaks
    later, you know it was working at some point.
  - Keep skill_taxonomy.py small (20-30 entries max) for the hackathon.
    It's easy to expand later.
  - If time runs critically short, the Python scorer can be temporarily
    replaced with a plain JavaScript function in Node.js using the same
    algorithm — the scoring math is simple. Ship the NLP version first,
    fall back only if absolutely necessary.
  - DO NOT skip Step 18 (deployment prep). Two Render services need two
    separate deploy configs — budget 1-2 hours for this.

================================================================================
